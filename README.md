# ðŸ§  HTHTA: A Hybrid Temporalâ€“Hierarchical Transformer-Attention Framework for Cross-Domain Time-Sensitive Prediction

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Transformer](https://img.shields.io/badge/Architecture-Transformer--Hybrid-purple)
![Status](https://img.shields.io/badge/Status-Under%20Review-orange)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

> **HTHTA is the first unified deep learning framework that generalizes across domainsâ€”solar forecasting, text classification, and clinical predictionâ€”without redesigning architecture per task.**

---

## ðŸ” Abstract

HTHTA integrates three core components:

- ðŸŒ² **Random Forest-based temporal embedding** for structured tabular time-series
- ðŸ¤– **BERT encoder** for text semantics
- ðŸ§  **BiLSTM + Hierarchical Attention** for universal downstream prediction

It achieves:
- ðŸ“‰ RMSE `0.00048 kW` in solar forecasting (29% better than CNN-LSTM)
- ðŸ“° 94.6% accuracy on AG News (0.3% > RoBERTa)
- ðŸ©º AUC `0.87` for heart failure prognosis (XGBoost = 0.85)

---

## âš™ï¸ Methodology

```mermaid
graph TD
    A1[Structured Data] --> B1[Random Forest Embedding]
    A2[Text Data] --> B2[BERT Encoder]
    B1 --> C[BiLSTM Layer]
    B2 --> C
    C --> D[Hierarchical Attention]
    D --> E[Prediction Head (Regression or Classification)]
